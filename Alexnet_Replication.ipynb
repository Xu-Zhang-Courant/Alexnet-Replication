{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replication of Alexnet. \n",
    "#### Due to limited computational resources the replciation is in limited scale. In particular, only the network were trained to only recognize first 20 classes of images. But this can be easily relaxed given sufficient computational power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bk0WVR_yIeeD",
    "outputId": "22704eac-dace-4f1c-e970-f7f0cae2ad38"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import partial\n",
    "from typing import Any, Optional\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import torch.nn.init as init\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Architecture.\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes: int = 1000, dropout: float = 0.5) -> None:\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(96, 256, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    # Xavier Initialization\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                # Initialize weights for convolutional and linear layers\n",
    "                init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    # Initialize biases if they exist\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle Exception for greyscale images\n",
    "class ConvertToRGB(object):\n",
    "    def __call__(self, img):\n",
    "        if img.shape[0] == 1:  # Check if the image has only one channel\n",
    "            img = torch.stack([img[0]] * 3, dim=0)  # Convert single channel to RGB\n",
    "        return img \n",
    "\n",
    "# Preprocessing Images\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    ConvertToRGB(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the images were stored locally\n",
    "image_path = \"/Users/Limit/imagenet-object-localization-challenge_100\"\n",
    "filenames_image_path = []\n",
    "label_train = []\n",
    "root_image = []\n",
    "counter = 0\n",
    "current_label = 0\n",
    "\n",
    "for root, _, filenames in os.walk(image_path):\n",
    "    current_root = root\n",
    "    for i in filenames:\n",
    "        counter += 1\n",
    "        # Print whenever one classes of images finished reading (each class has 1300 images)\n",
    "        if ((counter) %1300 == 0):\n",
    "            current_label += 1\n",
    "            print(counter)\n",
    "        # get labels\n",
    "        label_train.append(current_label)\n",
    "        temp = current_root + \"\\\\\" + i\n",
    "        filenames_image_path.append(temp)\n",
    "true_label = 0    \n",
    "correct_labels = 0\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"image loading complete\")\n",
    "\n",
    "counter_1=0\n",
    "x_train = []\n",
    "# Due to limited computational resources, only train with first 20 classes of images.\n",
    "for i in range(26000):\n",
    "    image_name = filenames_image_path[i] \n",
    "    input_image = Image.open(image_name)\n",
    "    input_tensor = preprocess(input_image)\n",
    "    input_batch = input_tensor\n",
    "    # move the input and model to GPU for speed if available\n",
    "    if torch.cuda.is_available():\n",
    "        input_batch = input_batch.to('cuda')\n",
    "        model_alex_given.to('cuda')\n",
    "    \n",
    "    x_train.append(input_batch)\n",
    "    counter_1 += 1\n",
    "    # print the counter whenever finished processing the corresponding class\n",
    "    if ((counter_1+1) %1300 == 0):\n",
    "            counter_1 += 1\n",
    "            print(counter_1)\n",
    "    \n",
    "print('image processing compelte')\n",
    "\n",
    "y_train = label_train[:26000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], torch.tensor(self.labels[idx])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Initialize model\n",
    "model = AlexNet().to(device)\n",
    "model.initialize_weights()\n",
    "# Define the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "# Prepare the data\n",
    "train_data = x_train  # List of input tensors\n",
    "train_labels = y_train  # List of corresponding labels\n",
    "dataset = CustomDataset(train_data, train_labels)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Train the models\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    current_loss = 0.0\n",
    "    for inputs, labels in dataloader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        # get the gradient before the update\n",
    "        Before = list(model.parameters())[0].clone()\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        current_loss += loss.item()\n",
    "        # get the gradient after the update\n",
    "        After = list(model.parameters())[0].clone()\n",
    "    # Print the current learning detail    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(\"Learning rate:\", param_group['lr'])\n",
    "    print()\n",
    "    print('another way to print learning rate:')\n",
    "    for group in optimizer.param_groups:\n",
    "        for p in group['params']:\n",
    "            print(p.grad)  # Print gradients\n",
    "    print('end of p.grad')\n",
    "    print()\n",
    "    # Verify whether gradient is computed successfully\n",
    "    print(torch.equal(Before.data, After.data))\n",
    "    print(f'Epoch {epoch+1} finished')\n",
    "    epoch_loss = current_loss / len(dataset)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    print('**************************************************************')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read test_label location\n",
    "labels_path = '/Users/Limit/imagenet_annot/validation_set_labels.csv'\n",
    "labels_df = pd.read_csv(labels_path)\n",
    "# find the labels of only the first 20 classes\n",
    "labels_df_leq_20 = labels_df[labels_df['label'] <= 20]\n",
    "labels_validation_images = labels_df_leq_20['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "# find the test images\n",
    "image_path = \"/Users/Limit/imagenet-object-localization-challenge_validation/val\"\n",
    "filenames_image_path = []\n",
    "for root, _, filenames in os.walk(image_path):\n",
    "    for i in filenames:\n",
    "        if (i.split('.')[0] in labels_df_leq_20['ImageId'].tolist()):\n",
    "            filenames_image_path.append(i)\n",
    "true_label = 0    \n",
    "counter = 0\n",
    "correct_labels = 0\n",
    "start_time = time.time()\n",
    "grab_980_max_val = []\n",
    "for i in range(len(filenames_image_path)):\n",
    "    counter +=1\n",
    "    image_name = image_path + '/' + filenames_image_path[i]\n",
    "    input_image = Image.open(image_name)\n",
    "\n",
    "    input_tensor = preprocess(input_image)\n",
    "    input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "\n",
    "    # move the input and model to GPU for speed if available\n",
    "    if torch.cuda.is_available():\n",
    "        input_batch = input_batch.to('cuda')\n",
    "        model.to('cuda')\n",
    "    \n",
    "    if (counter%100 == 0):\n",
    "        print(\"currently at\", counter, 'current time is', time.time() - start_time)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_batch)\n",
    "\n",
    "    # print the prediction results in this format\n",
    "    print('********************************************************************************')\n",
    "    print('Predicting Test Sample', counter, ':   Prediction is Correct?')\n",
    "    if (torch.argmax(output[0]).item() == labels_validation_images[i]):\n",
    "        print('Yes')\n",
    "        correct_labels += 1\n",
    "    else:\n",
    "        print('No')\n",
    "    prob_softmax = torch.softmax(output[0], dim = 0)\n",
    "    print(\"first 20 classes probability:\", prob_softmax[:20])\n",
    "    print()\n",
    "    print('max probability is', torch.max(prob_softmax, dim = 0))\n",
    "    print()\n",
    "    print('the max probability of the rest of 980 dim is')\n",
    "    print(torch.topk(prob_softmax[20:], k=4))\n",
    "    print('End*****************************************************************************')\n",
    "    print()\n",
    "    \n",
    "    grab_980_max_val.append(torch.topk(prob_softmax[20:], k=4)[1][1:])\n",
    "    # The output has unnormalized scores. To get probabilities, can run a softmax on it.\n",
    "    probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "\n",
    "print('the overall testing error is')\n",
    "print(correct_labels/counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weight as the output\n",
    "torch.save(model.state_dict(), 'model_weights_alexnet_replication.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
